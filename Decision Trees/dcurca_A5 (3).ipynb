{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Decision Trees and Ensemble Methods [ __ / 70  marks]\n",
    "\n",
    "<img src=\"https://datasciencetoday.net/images/2018/11/27/tree.png\">\n",
    "\n",
    "For this assignment we will use standalone and ensembled decision trees (Bagging, AdaBoost) in order to predict whether particular red wines are `high quality` or `low quality` based on some associated input features (e.g., fixed acidity, residual sugar, density, alcohol, etc). \n",
    "\n",
    "We will first import our data. Next, we will apply the pre-processing steps. Finally, we will construct and compare models. There will also be some communication questions along the way. \n",
    "\n",
    "I hope this week's lesson and this assignment will make you more comfortable with using and thinking about decision trees --- they're a very powerful tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start...\n",
    "* check out the relevant lecture code for reference (`L9_CF.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you submit...\n",
    "* restart the kernel, then re-run the whole notebook to ensure no errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 [ _ /3 marks]\n",
    "\n",
    "Read the file `winequality-red.csv` into a pandas DataFrame. Display the first 5 rows of the DataFrame. Make sure to remove the semicolons.\n",
    "\n",
    "`Hint`: Sometimes you will see that dataset entries are separated by `;`,`,`,`&`. You can use the `sep` argument in `read_csv()` to format it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ****** your code here ******\n",
    "df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 [ _ /7 marks]\n",
    "\n",
    "Before building our models, we will need to **preprocess** the data. Instead of using the 10-class column ('quality') directly, let's just focus on classifying red wines as `'high quality'` or `'low quality'` by manually assigning a threshold. We will consider wines with 'quality' 7 or higher as **'high quality'** (class label `1`) and those with 'quality' 6 or lower as **'low quality'** (class label `0`). Replace the `quality` column with your new column (`CLASS`). \n",
    "\n",
    "Display the first 5 rows of the new dataframe. How many instances of class `0` and class `1` are there? [ /1 mark] Is the data class-balanced? [ /1 mark] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1382\n",
       "1     217\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace dependent variable 'quality' with `CLASS` (labels 0 and 1) [ /4 marks]\n",
    "df = df.rename(columns={\"quality\": \"CLASS\"})\n",
    "df['CLASS'] = np.where(df['CLASS']>6, 1, 0)\n",
    "df.head()\n",
    "\n",
    "# How many instances of class 0 and class 1 are there in the data? (code) [ /1 mark]\n",
    "df.CLASS.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: There are 1382 instances of \"low quality\" wine where CLASS = 0 and there are 217 instances of \"high quality\" wine where CLASS = 1. This means that the data is class unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3 [ _ /3 marks]\n",
    "\n",
    "Let's create our train and test sets. Create an input dataframe X (the input features); next, create an output series y which contains the output class labels (a column of `0`'s and `1`'s). Split the data into train and test sets with `train_test_split`. Use `test_size = 0.3`, `random_state = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the feature data into X; store the class data into y [ /2 marks]\n",
    "# ****** your code here ******\n",
    "X = df.drop(\"CLASS\", axis=1)\n",
    "y = df.CLASS\n",
    "\n",
    "# Do a train-test split. Use 30% of the data for testing. Use random state 0. [ /1 mark]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.3,random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 [ _ /13 marks]\n",
    "\n",
    "For our first model we will **select a standalone decision tree of optimal maximum depth** (out of possible maximum depths ranging from `2 to 30`). \n",
    "\n",
    "To find the optimal maximum depth, create multiple decision trees with sklearn's `DecisionTreeClassifier` class (you can use a loop), then compute the mean Cross-Validation score for each (use 5-fold CV). You don't need to specify a scorer (Note: for `DecisionTreeClassifier` it's *mean accuracy*).\n",
    "\n",
    "Create a plot which shows the mean CV scores on the y-axis and maximum depths on the x-axis. **Report the optimal maximum depth**. [ /2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CV Scores')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and fit trees from max_depth 2 to max_depth 30. Use 5-fold CV for each. [ /8 marks]\n",
    "list_scores = []\n",
    "list_depths = []\n",
    "for i in range(2,30):\n",
    "    tree = DecisionTreeClassifier(max_depth=i)\n",
    "    scores = cross_val_score(tree, Xtrain, ytrain, cv=5).mean()\n",
    "    list_depths.append(i)\n",
    "    list_scores.append(scores)\n",
    " \n",
    "# Plot the mean CV score vs. maximum depth [ /3 marks]\n",
    "# ****** your code here ******\n",
    "plt.scatter(list_depths, list_scores)\n",
    "plt.title(\"CV Score vs Maximum Depth\")\n",
    "plt.xlabel(\"Maximum Depths\")\n",
    "plt.ylabel(\"CV Scores\")\n",
    "\n",
    "# Report the optimal max_depth either here or in the markdown cell below. [ /2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: The optimal maximum depth is the highest possible cv score in our case depth 4 with a cv score of approximately 0.87490."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 [ _ /6 marks]\n",
    "\n",
    "Consider your optimal max_depth tree (or, you can create a new one with that depth). Fit it to the training set. Report its test accuracy on the test set.\n",
    "\n",
    "Hint: You can use `accuracy_score()` here to report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.8958333333333334\n"
     ]
    }
   ],
   "source": [
    "# Fit your optimal-depth tree, then calculate test accuracy [ /6 marks]\n",
    "new_tree = DecisionTreeClassifier(max_depth=4)\n",
    "new_tree.fit(Xtrain, ytrain)\n",
    "ypred = new_tree.predict(Xtest)\n",
    "\n",
    "print(f\"Accuracy Score = {accuracy_score(ytest, ypred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 [ _ /4 marks] \n",
    "\n",
    "What is the **major shortcoming** of standalone decision trees? [ /2 marks] What is the **purpose of creating an ensemble of trees**? [ /2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**: The major shortcoming of standalone decision trees is that they have high variance due to overfitting. The purpose of creating an ensemble of trees is that they provide a solution on how to reduce the high variance of standalone decision trees. Normally, several decision trees are combined to produce better predictive performance than just 1 decision tree, the main principle is that a group of weak trees/predictions can be combined to produce a strong prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1 [ _ /24 marks]\n",
    "\n",
    "Let's now focus on creating an **ensemble of 500 trees**. For this question we'll consider Bagging (Bootstrap Aggregation). Follow these steps:\n",
    "* **Step 1**: Create 500 Bootstrap samples (i.e. sample with replacement) from the dataset (specifically, sample from the training data).\n",
    "* **Step 2**: Train a particular tree (`max_depth=4`) on each Bootstrap sample (you'll therefore need 500 trees in total)\n",
    "* **Step 3**: Compute the **overall prediction** of your ensemble on the unseen test set. The overall prediction from each individual test input will come from a vote count from each of the 500 trees. **Report the test accuracy**.\n",
    "\n",
    "To expand on the `voting` point from Step 3: For each test input, each tree will make a certain output prediction. So, for a single test input you'll have 500 votes, and these could be 1 or 0. For the overall prediction for that single test input, you'll count which class (0 or 1) got the most votes.\n",
    "\n",
    "Note: Since this question is meant to be a manually done, you won't get the marks if you use sklearn's `BaggingClassifier` or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.9083333333333333\n"
     ]
    }
   ],
   "source": [
    "# Create 500 Bootstrap samples from the training set. Fit a tree to each [ /12 marks]\n",
    "df2 = Xtrain.join(ytrain) #join Xtrain with ytrain data to new dataframe\n",
    "\n",
    "def bootstrap(S):\n",
    "    nsim = 500\n",
    "    tree_list = []\n",
    "    \n",
    "    for i in range(nsim):\n",
    "        bootstrap_samples = S.sample(nsim, replace=True)\n",
    "        model = DecisionTreeClassifier(max_depth=4)\n",
    "        model.fit(bootstrap_samples.drop(columns=\"CLASS\"),bootstrap_samples[\"CLASS\"])\n",
    "        tree_list.append(model)\n",
    "    return tree_list\n",
    "\n",
    "# Make 500 predictions on the test data (unseen by your trees so far). [ /6 marks] \n",
    "ypr = []\n",
    "samples = bootstrap(df2)\n",
    "for j in range(0, len(samples)):\n",
    "    ypr.append(samples[j].predict(Xtest))\n",
    "\n",
    "# Finally, compute the overall vote for each prediction (the most votes for a given class wins) [ /4 marks]\n",
    "# Hint: If there's a tie, the common way is to predict the class with the lowest class label\n",
    "# However, it's also ok to use scipy.stats.mode here (this randomly picks tie winners)\n",
    "votes = []\n",
    "    \n",
    "for l in range(0, len(ypr[0])):\n",
    "    ones_pred = 0\n",
    "    zero_pred = 0\n",
    "    for m in range(0, len(ypr)):\n",
    "        if ypr[m][l]:\n",
    "            ones_pred += 1\n",
    "        else:\n",
    "            zero_pred += 1\n",
    "    \n",
    "    if ones_pred > zero_pred:\n",
    "        votes.append(1)\n",
    "    else:\n",
    "        votes.append(0)\n",
    "\n",
    "# Report (print) the accuracy of your ensemble model on the test set. Use accuracy_score() [ /2 marks]\n",
    "print(f\"Accuracy Score = {accuracy_score(ytest, votes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2 [ _ /6 marks]\n",
    "\n",
    "Finally, let's consider AdaBoost. Here, each tree (a stump) is trained sequentially and relies on the previous tree for its training data (which was re-sampled, and this was influenced by the sample weight changes as a result of incorrect predictions from the previous tree). \n",
    "\n",
    "Create an `AdaBoostClassifier` object with `base_estimator = DecisionTreeClassifier(max_depth=4)`, `n_estimators=500`, `learning_rate=0.1`. Fit to the training data, and compute (and report) the test accuracy. You can use `accuracy_score()` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# Create an AdaBoostClassifier object with the specified arguments [ /2 marks]\n",
    "ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=4), n_estimators=500, learning_rate=0.1)\n",
    "\n",
    "# Fit to the training data and compute the test predictions [ /2 marks]\n",
    "ada.fit(Xtrain, ytrain)\n",
    "ypred3 = ada.predict(Xtest)\n",
    "\n",
    "# Compute and report the test accuracy [ /2 marks]\n",
    "print(f\"Accuracy Score = {accuracy_score(ytest, ypred3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3 [ _ /4 marks]\n",
    "\n",
    "Finally, compare the test accuracies of the models considered so far: the optimal max_depth Decision Tree, the bagged ensemble of 500 trees, and the AdaBoost ensemble of 500 trees. Which performed worst? Which performed best? Do the results agree with our intuition from the Lecture?\n",
    "\n",
    "Hint: One intuition you could use from the lecture was that ensemble methods have **lower variance** than a single tree (i.e. less prone to overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**:The standalone Decision Tree Classifier had the lowest accuracy score of around 0.89 therefore it performed the worst while both of the ensemble methods had higher accuracy scores of approximately 0.90 and 0.91.The ada classifier performed the best with approximately 0.9166 accuracy. This fits with our intuition because we know that standalone decision trees have high variance or are prone to overfitting. This means that the model is overfitting to the training data and thus is fair to assume the model will perform worse when given the test dataset compared to a model that is more flexible to the variation or noise in the data. When overfitting we typically fit to the noise or variations within the data which is not uniform data, therefore the model will generally perform worse. This is evident in our calculations of accuracy score, a standalone Decision Tree Classifier had the lowest accuracy score which correlates with overfitting and high variance. Adaboost as well as bootstrap had a higher accuracy score which shows us that these models are less prone to overfitting demostrating their lower variance compared to standalone decision trees since they are more flexible to noise/variations within the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
